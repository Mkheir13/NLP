"""
Test du pipeline sur un mini Ã©chantillon de 50 lignes avec tables de sauvegarde
"""

import sys
import os
sys.path.append('src')

from src.massive_preprocessing_pipeline import MassivePreprocessingPipeline
import time
import pandas as pd
import json
from pathlib import Path

def test_mini_sample():
    """
    Test le pipeline sur seulement 50 Ã©chantillons avec tables de sauvegarde
    """
    print("ğŸ§ª TEST MINI-Ã‰CHANTILLON - 50 LIGNES AVEC SAUVEGARDES")
    print("=" * 70)
    print("Test sur 50 Ã©chantillons pour voir le fonctionnement dÃ©taillÃ©")
    print("Avec tables de sauvegarde au cas oÃ¹")
    print("=" * 70)
    
    # Configuration pour mini test
    output_dir = "data/mini_test"
    pipeline = MassivePreprocessingPipeline(
        output_dir=output_dir,
        use_multiprocessing=False,  # Pas de multiprocessing pour 50 Ã©chantillons
        chunk_size=25  # TrÃ¨s petits chunks
    )
    
    # CrÃ©er le dossier de sauvegarde
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # Charger seulement 50 Ã©chantillons
    from datasets import load_dataset
    
    print("ğŸ”„ Chargement d'un mini Ã©chantillon...")
    start_time = time.time()
    
    # Charger seulement 50 Ã©chantillons train et 10 test
    train_dataset = load_dataset("fancyzhx/amazon_polarity", split="train[:50]")
    test_dataset = load_dataset("fancyzhx/amazon_polarity", split="test[:10]")
    
    load_time = time.time() - start_time
    print(f"âœ… Mini Ã©chantillon chargÃ© en {load_time:.1f}s")
    print(f"  - Train: {len(train_dataset)} Ã©chantillons")
    print(f"  - Test: {len(test_dataset)} Ã©chantillons")
    
    # SAUVEGARDE 1: DonnÃ©es brutes
    print("\nğŸ’¾ SAUVEGARDE 1: DonnÃ©es brutes")
    raw_train = pd.DataFrame(train_dataset)
    raw_test = pd.DataFrame(test_dataset)
    
    raw_train.to_csv(f"{output_dir}/raw_train_backup.csv", index=False)
    raw_test.to_csv(f"{output_dir}/raw_test_backup.csv", index=False)
    print(f"  âœ… SauvegardÃ©: raw_train_backup.csv ({len(raw_train)} lignes)")
    print(f"  âœ… SauvegardÃ©: raw_test_backup.csv ({len(raw_test)} lignes)")
    
    # Traiter les donnÃ©es
    print("\nğŸ“š TRAITEMENT DU MINI TRAIN")
    print("-" * 40)
    train_df = pipeline.process_dataset_split(train_dataset, 'mini_train')
    
    # SAUVEGARDE 2: Train preprocessÃ©
    print("\nğŸ’¾ SAUVEGARDE 2: Train preprocessÃ©")
    train_df.to_csv(f"{output_dir}/processed_train_backup.csv", index=False)
    train_df.to_parquet(f"{output_dir}/processed_train_backup.parquet", index=False)
    print(f"  âœ… SauvegardÃ©: processed_train_backup.csv ({len(train_df)} lignes)")
    print(f"  âœ… SauvegardÃ©: processed_train_backup.parquet")
    
    print("\nğŸ§ª TRAITEMENT DU MINI TEST")
    print("-" * 40)
    test_df = pipeline.process_dataset_split(test_dataset, 'mini_test')
    
    # SAUVEGARDE 3: Test preprocessÃ©
    print("\nğŸ’¾ SAUVEGARDE 3: Test preprocessÃ©")
    test_df.to_csv(f"{output_dir}/processed_test_backup.csv", index=False)
    test_df.to_parquet(f"{output_dir}/processed_test_backup.parquet", index=False)
    print(f"  âœ… SauvegardÃ©: processed_test_backup.csv ({len(test_df)} lignes)")
    print(f"  âœ… SauvegardÃ©: processed_test_backup.parquet")
    
    # Sauvegarder via le pipeline standard
    pipeline.save_processed_data(train_df, test_df)
    
    # SAUVEGARDE 4: Analyse dÃ©taillÃ©e
    print("\nğŸ’¾ SAUVEGARDE 4: Analyse dÃ©taillÃ©e")
    
    # Analyse des colonnes
    columns_info = {
        'total_columns': len(train_df.columns),
        'column_names': list(train_df.columns),
        'data_types': {col: str(train_df[col].dtype) for col in train_df.columns}
    }
    
    # Statistiques dÃ©taillÃ©es
    detailed_stats = {
        'dataset_size': {
            'train': len(train_df),
            'test': len(test_df),
            'total': len(train_df) + len(test_df)
        },
        'columns_info': columns_info,
        'text_analysis': {
            'avg_original_length': float(train_df['text_length'].mean()),
            'max_original_length': int(train_df['text_length'].max()),
            'min_original_length': int(train_df['text_length'].min()),
            'avg_processed_length': float(train_df['processed_length'].mean()),
            'compression_ratio': float(1 - (train_df['processed_length'].mean() / train_df['text_length'].mean()))
        },
        'sentiment_analysis': {
            'avg_polarity': float(train_df['polarity'].mean()),
            'avg_subjectivity': float(train_df['subjectivity'].mean()),
            'polarity_range': [float(train_df['polarity'].min()), float(train_df['polarity'].max())]
        },
        'label_quality': {
            'train_suspect_count': int(train_df['is_label_suspect'].sum()),
            'test_suspect_count': int(test_df['is_label_suspect'].sum()),
            'train_suspect_ratio': float(train_df['is_label_suspect'].mean()),
            'test_suspect_ratio': float(test_df['is_label_suspect'].mean())
        }
    }
    
    with open(f"{output_dir}/detailed_analysis.json", 'w', encoding='utf-8') as f:
        json.dump(detailed_stats, f, indent=2, ensure_ascii=False)
    
    print(f"  âœ… SauvegardÃ©: detailed_analysis.json")
    
    # Afficher les rÃ©sultats dÃ©taillÃ©s
    print("\nğŸ¯ RÃ‰SULTATS DÃ‰TAILLÃ‰S DU MINI TEST:")
    print("=" * 50)
    print(f"ğŸ“Š DonnÃ©es traitÃ©es:")
    print(f"  - Train: {len(train_df)} Ã©chantillons")
    print(f"  - Test: {len(test_df)} Ã©chantillons")
    print(f"  - Features extraites: {len(train_df.columns)} colonnes")
    
    print(f"\nğŸ“ Colonnes crÃ©Ã©es:")
    for i, col in enumerate(train_df.columns, 1):
        print(f"  {i:2d}. {col}")
    
    print(f"\nğŸ“Š Statistiques texte:")
    print(f"  - Longueur moyenne originale: {train_df['text_length'].mean():.1f} caractÃ¨res")
    print(f"  - Longueur moyenne preprocessÃ©e: {train_df['processed_length'].mean():.1f} caractÃ¨res")
    print(f"  - Ratio de compression: {(1 - train_df['processed_length'].mean()/train_df['text_length'].mean())*100:.1f}%")
    
    print(f"\nğŸ­ Analyse de sentiment:")
    print(f"  - PolaritÃ© moyenne: {train_df['polarity'].mean():.3f}")
    print(f"  - SubjectivitÃ© moyenne: {train_df['subjectivity'].mean():.3f}")
    
    print(f"\nğŸš¨ QualitÃ© des labels:")
    print(f"  - Erreurs dÃ©tectÃ©es (train): {train_df['is_label_suspect'].sum()}/{len(train_df)} ({train_df['is_label_suspect'].mean()*100:.1f}%)")
    print(f"  - Erreurs dÃ©tectÃ©es (test): {test_df['is_label_suspect'].sum()}/{len(test_df)} ({test_df['is_label_suspect'].mean()*100:.1f}%)")
    
    # Afficher quelques exemples d'erreurs dÃ©tectÃ©es si il y en a
    errors = train_df[train_df['is_label_suspect'] == True]
    if len(errors) > 0:
        print(f"\nğŸš¨ EXEMPLES D'ERREURS DÃ‰TECTÃ‰ES:")
        print("-" * 50)
        
        for i, (_, row) in enumerate(errors.head(3).iterrows()):
            print(f"\n{i+1}. Label original: {'Positif' if row['label'] == 1 else 'NÃ©gatif'}")
            print(f"   PolaritÃ© calculÃ©e: {row['polarity']:.3f}")
            print(f"   Raison: {row['suspect_reason']}")
            print(f"   Titre: {row['title'][:80]}...")
            print(f"   Contenu: {row['content'][:100]}...")
    else:
        print(f"\nâœ… Aucune erreur de label dÃ©tectÃ©e dans ce mini Ã©chantillon")
    
    # Afficher quelques exemples de preprocessing
    print(f"\nğŸ” EXEMPLES DE PREPROCESSING:")
    print("-" * 50)
    for i in range(min(3, len(train_df))):
        row = train_df.iloc[i]
        print(f"\n{i+1}. Ã‰chantillon:")
        print(f"   Original ({len(row['combined_text'])} chars): {row['combined_text'][:100]}...")
        print(f"   PreprocessÃ© ({len(row['processed_text'])} chars): {row['processed_text'][:100]}...")
        print(f"   Features: {row['word_count']} mots, {row['sentence_count']} phrases")
    
    print(f"\nğŸ’¾ FICHIERS DE SAUVEGARDE CRÃ‰Ã‰S:")
    print("-" * 50)
    backup_files = [
        "raw_train_backup.csv",
        "raw_test_backup.csv", 
        "processed_train_backup.csv",
        "processed_train_backup.parquet",
        "processed_test_backup.csv",
        "processed_test_backup.parquet",
        "train_processed.csv",
        "test_processed.csv",
        "train_processed.parquet",
        "test_processed.parquet",
        "train_processed.pkl",
        "test_processed.pkl",
        "processing_stats.json",
        "processing_summary.json",
        "detailed_analysis.json"
    ]
    
    for filename in backup_files:
        filepath = Path(output_dir) / filename
        if filepath.exists():
            size_kb = filepath.stat().st_size / 1024
            print(f"  âœ… {filename} ({size_kb:.1f} KB)")
        else:
            print(f"  âŒ {filename} (non crÃ©Ã©)")
    
    print(f"\nâœ… MINI TEST TERMINÃ‰!")
    print(f"ğŸ“ Tous les fichiers sont dans: {output_dir}/")
    print(f"ğŸ¯ Le pipeline fonctionne correctement sur le mini Ã©chantillon")
    
    return train_df, test_df

def main():
    """
    Lance le mini test
    """
    try:
        print("ğŸš€ LANCEMENT DU MINI TEST (50 Ã‰CHANTILLONS)")
        
        train_df, test_df = test_mini_sample()
        
        print(f"\nğŸ‰ MINI TEST RÃ‰USSI!")
        print(f"Le pipeline est prÃªt pour des Ã©chantillons plus grands.")
        print(f"\nPour tester sur 10k Ã©chantillons:")
        print(f"  python test_massive_pipeline.py")
        print(f"\nPour le preprocessing complet:")
        print(f"  python run_full_preprocessing.py")
        
    except Exception as e:
        print(f"\nâŒ ERREUR LORS DU MINI TEST: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 