"""
Test du pipeline massif avec un Ã©chantillon plus petit
"""

import sys
sys.path.append('src')

from src.massive_preprocessing_pipeline import MassivePreprocessingPipeline
import time

def test_pipeline_sample():
    """
    Test le pipeline sur un Ã©chantillon de 10,000 Ã©chantillons
    """
    print("ğŸ§ª TEST DU PIPELINE MASSIF - Ã‰CHANTILLON")
    print("=" * 60)
    print("Test sur 10,000 Ã©chantillons pour vÃ©rifier le fonctionnement")
    print("=" * 60)
    
    # Configuration pour test
    pipeline = MassivePreprocessingPipeline(
        output_dir="data/test_processed",
        use_multiprocessing=True,
        chunk_size=2000  # Chunks plus petits pour le test
    )
    
    # Modifier la mÃ©thode pour charger seulement un Ã©chantillon
    from datasets import load_dataset
    
    print("ğŸ”„ Chargement d'un Ã©chantillon de test...")
    start_time = time.time()
    
    # Charger seulement 10k Ã©chantillons
    train_dataset = load_dataset("fancyzhx/amazon_polarity", split="train[:10000]")
    test_dataset = load_dataset("fancyzhx/amazon_polarity", split="test[:1000]")
    
    load_time = time.time() - start_time
    print(f"âœ… Ã‰chantillon chargÃ© en {load_time:.1f}s")
    print(f"  - Train: {len(train_dataset):,} Ã©chantillons")
    print(f"  - Test: {len(test_dataset):,} Ã©chantillons")
    
    # Traiter les donnÃ©es
    print("\nğŸ“š TRAITEMENT DU SPLIT D'ENTRAÃNEMENT (Ã‰CHANTILLON)")
    train_df = pipeline.process_dataset_split(train_dataset, 'train_sample')
    
    print("\nğŸ§ª TRAITEMENT DU SPLIT DE TEST (Ã‰CHANTILLON)")
    test_df = pipeline.process_dataset_split(test_dataset, 'test_sample')
    
    # Sauvegarder
    pipeline.save_processed_data(train_df, test_df)
    
    # Afficher les rÃ©sultats
    print("\nğŸ¯ RÃ‰SULTATS DU TEST:")
    print("=" * 40)
    print(f"Train preprocessÃ©: {len(train_df):,} Ã©chantillons")
    print(f"Test preprocessÃ©: {len(test_df):,} Ã©chantillons")
    print(f"Features extraites: {len(train_df.columns)} colonnes")
    print(f"Erreurs dÃ©tectÃ©es (train): {train_df['is_label_suspect'].sum():,}")
    print(f"Erreurs dÃ©tectÃ©es (test): {test_df['is_label_suspect'].sum():,}")
    print(f"Ratio d'erreurs (train): {train_df['is_label_suspect'].mean()*100:.2f}%")
    print(f"Ratio d'erreurs (test): {test_df['is_label_suspect'].mean()*100:.2f}%")
    
    # Afficher quelques exemples d'erreurs dÃ©tectÃ©es
    print(f"\nğŸš¨ EXEMPLES D'ERREURS DÃ‰TECTÃ‰ES:")
    print("-" * 40)
    
    errors = train_df[train_df['is_label_suspect'] == True].head(3)
    for i, (_, row) in enumerate(errors.iterrows()):
        print(f"\n{i+1}. Label: {'Positif' if row['label'] == 1 else 'NÃ©gatif'}")
        print(f"   PolaritÃ©: {row['polarity']:.3f}")
        print(f"   Raison: {row['suspect_reason']}")
        print(f"   Texte: {row['combined_text'][:100]}...")
    
    print(f"\nâœ… TEST TERMINÃ‰! Le pipeline fonctionne correctement.")
    print(f"ğŸ“ RÃ©sultats sauvegardÃ©s dans: data/test_processed/")
    
    return train_df, test_df

def main():
    """
    Lance le test du pipeline
    """
    try:
        train_df, test_df = test_pipeline_sample()
        
        print(f"\nğŸ‰ TEST RÃ‰USSI!")
        print(f"Le pipeline est prÃªt pour traiter l'intÃ©gralitÃ© du dataset.")
        print(f"\nPour lancer le preprocessing complet:")
        print(f"  cd src && python massive_preprocessing_pipeline.py")
        
    except Exception as e:
        print(f"\nâŒ ERREUR LORS DU TEST: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 