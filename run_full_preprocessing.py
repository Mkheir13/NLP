"""
Script automatisÃ© pour lancer le preprocessing complet sur l'intÃ©gralitÃ© du dataset
"""

import sys
import time
sys.path.append('src')

from src.massive_preprocessing_pipeline import MassivePreprocessingPipeline

def main():
    """
    Lance le preprocessing complet automatiquement
    """
    print("ğŸš€ PREPROCESSING AUTOMATIQUE DE L'INTÃ‰GRALITÃ‰ DU DATASET AMAZON POLARITY")
    print("=" * 80)
    print("ğŸ“Š Dataset Ã  traiter:")
    print("  - 3,600,000 Ã©chantillons d'entraÃ®nement")
    print("  - 400,000 Ã©chantillons de test")
    print("  - Total: 4,000,000 Ã©chantillons")
    print("=" * 80)
    print("ğŸ”§ Configuration:")
    print("  - Preprocessing complet avec toutes les features")
    print("  - DÃ©tection et flagging des erreurs de labels")
    print("  - Multiprocessing optimisÃ©")
    print("  - Sauvegarde en multiple formats")
    print("=" * 80)
    
    start_time = time.time()
    
    # Configuration du pipeline pour traitement complet
    pipeline = MassivePreprocessingPipeline(
        output_dir="data/processed_full",
        use_multiprocessing=True,
        chunk_size=5000  # Chunks optimisÃ©s pour 4M d'Ã©chantillons
    )
    
    try:
        print("ğŸš€ DÃ‰BUT DU TRAITEMENT COMPLET...")
        print("â±ï¸  Temps estimÃ©: 2-3 heures")
        print("ğŸ“Š Vitesse attendue: ~450 Ã©chantillons/seconde")
        print("-" * 80)
        
        # Lancer le pipeline complet
        train_df, test_df = pipeline.run_full_pipeline()
        
        total_time = time.time() - start_time
        
        print("\n" + "=" * 80)
        print("ğŸ‰ PREPROCESSING COMPLET TERMINÃ‰ AVEC SUCCÃˆS!")
        print("=" * 80)
        print(f"â±ï¸  Temps total: {total_time:.1f}s ({total_time/3600:.2f} heures)")
        print(f"ğŸ“Š Ã‰chantillons traitÃ©s: {len(train_df) + len(test_df):,}")
        print(f"âš¡ Vitesse moyenne: {(len(train_df) + len(test_df))/total_time:.1f} Ã©chantillons/seconde")
        print()
        print("ğŸ“ˆ RÃ‰SULTATS FINAUX:")
        print(f"  ğŸ”¸ Train: {len(train_df):,} Ã©chantillons")
        print(f"  ğŸ”¸ Test: {len(test_df):,} Ã©chantillons")
        print(f"  ğŸ”¸ Features: {len(train_df.columns)} colonnes")
        print(f"  ğŸ”¸ Erreurs dÃ©tectÃ©es (train): {train_df['is_label_suspect'].sum():,} ({train_df['is_label_suspect'].mean()*100:.2f}%)")
        print(f"  ğŸ”¸ Erreurs dÃ©tectÃ©es (test): {test_df['is_label_suspect'].sum():,} ({test_df['is_label_suspect'].mean()*100:.2f}%)")
        print()
        print("ğŸ’¾ DONNÃ‰ES SAUVEGARDÃ‰ES:")
        print("  ğŸ“ data/processed_full/train_processed.parquet")
        print("  ğŸ“ data/processed_full/test_processed.parquet")
        print("  ğŸ“ data/processed_full/train_processed.csv")
        print("  ğŸ“ data/processed_full/test_processed.csv")
        print("  ğŸ“ data/processed_full/processing_summary.json")
        print()
        print("âœ… LE DATASET EST PRÃŠT POUR LA MODÃ‰LISATION!")
        print("ğŸ¯ Prochaine Ã©tape: EntraÃ®nement des modÃ¨les de classification")
        
        return True
        
    except KeyboardInterrupt:
        print("\nâš ï¸  TRAITEMENT INTERROMPU PAR L'UTILISATEUR")
        print("ğŸ’¡ Le traitement peut Ãªtre repris plus tard")
        return False
        
    except Exception as e:
        print(f"\nâŒ ERREUR LORS DU TRAITEMENT: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    
    if success:
        print("\nğŸŠ FÃ‰LICITATIONS! Votre dataset est maintenant preprocessÃ© au niveau professionnel!")
        print("ğŸš€ Vous pouvez maintenant passer Ã  la modÃ©lisation avancÃ©e.")
    else:
        print("\nğŸ˜ Le preprocessing n'a pas pu Ãªtre terminÃ©.")
        print("ğŸ’¡ VÃ©rifiez les logs pour plus d'informations.") 