"""
Test du pipeline sur un mini Ã©chantillon de 50 lignes avec explications visuelles dÃ©taillÃ©es
"""

import sys
import os
sys.path.append('src')

from src.massive_preprocessing_pipeline import MassivePreprocessingPipeline
import time
import pandas as pd
import json
from pathlib import Path

def print_section(title, char="=", width=80):
    """Affiche une section avec style"""
    print(f"\n{char * width}")
    print(f"{title:^{width}}")
    print(f"{char * width}")

def print_subsection(title, char="-", width=60):
    """Affiche une sous-section avec style"""
    print(f"\n{char * width}")
    print(f" {title}")
    print(f"{char * width}")

def show_text_comparison(original, processed, max_length=100):
    """Affiche une comparaison visuelle avant/aprÃ¨s"""
    print(f"ğŸ“ AVANT  : {original[:max_length]}{'...' if len(original) > max_length else ''}")
    print(f"âœ¨ APRÃˆS : {processed[:max_length]}{'...' if len(processed) > max_length else ''}")
    print(f"ğŸ“Š STATS : {len(original)} â†’ {len(processed)} caractÃ¨res ({(1-len(processed)/len(original))*100:.1f}% compression)")

def visualize_features(row, index):
    """Affiche les features d'un Ã©chantillon de maniÃ¨re visuelle"""
    print(f"\nğŸ” Ã‰CHANTILLON #{index + 1}")
    print("â”Œ" + "â”€" * 78 + "â”")
    print(f"â”‚ ğŸ·ï¸  LABEL: {'âœ… POSITIF' if row['label'] == 1 else 'âŒ NÃ‰GATIF':<70} â”‚")
    print(f"â”‚ ğŸ“ LONGUEUR: {row['text_length']} â†’ {row['processed_length']} chars ({(1-row['processed_length']/row['text_length'])*100:.1f}% compression){'':>20} â”‚")
    print(f"â”‚ ğŸ“ MOTS: {row['word_count']} mots, {row['sentence_count']} phrases{'':>35} â”‚")
    print(f"â”‚ ğŸ­ SENTIMENT: PolaritÃ©={row['polarity']:.3f}, SubjectivitÃ©={row['subjectivity']:.3f}{'':>20} â”‚")
    print(f"â”‚ ğŸš¨ SUSPECT: {'âš ï¸  OUI - ' + row['suspect_reason'] if row['is_label_suspect'] else 'âœ… NON'}{'':>10} â”‚")
    print("â”œ" + "â”€" * 78 + "â”¤")
    print(f"â”‚ ğŸ“– TITRE: {row['title'][:70]:<70} â”‚")
    print("â”œ" + "â”€" * 78 + "â”¤")
    print(f"â”‚ ğŸ“„ ORIGINAL: {row['combined_text'][:66]:<66} â”‚")
    print(f"â”‚ âœ¨ PREPROCESSÃ‰: {row['processed_text'][:62]:<62} â”‚")
    print("â””" + "â”€" * 78 + "â”˜")

def create_visual_stats(train_df, test_df):
    """CrÃ©e des statistiques visuelles"""
    print_section("ğŸ“Š STATISTIQUES VISUELLES", "=")
    
    # Distribution des labels
    print_subsection("ğŸ·ï¸  DISTRIBUTION DES LABELS")
    train_pos = (train_df['label'] == 1).sum()
    train_neg = (train_df['label'] == 0).sum()
    test_pos = (test_df['label'] == 1).sum()
    test_neg = (test_df['label'] == 0).sum()
    
    print("TRAIN:")
    print(f"  Positifs: {'â–ˆ' * (train_pos // 2)}{train_pos:>3} ({train_pos/len(train_df)*100:.1f}%)")
    print(f"  NÃ©gatifs: {'â–ˆ' * (train_neg // 2)}{train_neg:>3} ({train_neg/len(train_df)*100:.1f}%)")
    
    print("\nTEST:")
    print(f"  Positifs: {'â–ˆ' * test_pos}{test_pos:>3} ({test_pos/len(test_df)*100:.1f}%)")
    print(f"  NÃ©gatifs: {'â–ˆ' * test_neg}{test_neg:>3} ({test_neg/len(test_df)*100:.1f}%)")
    
    # Histogramme des longueurs
    print_subsection("ğŸ“ DISTRIBUTION DES LONGUEURS DE TEXTE")
    lengths = train_df['text_length'].values
    min_len, max_len = lengths.min(), lengths.max()
    bins = 10
    hist, bin_edges = pd.cut(lengths, bins=bins, retbins=True)
    hist_counts = hist.value_counts().sort_index()
    
    print("Histogramme des longueurs (caractÃ¨res):")
    max_count = hist_counts.max()
    for i, (interval, count) in enumerate(hist_counts.items()):
        bar_length = int((count / max_count) * 30)
        left, right = int(interval.left), int(interval.right)
        print(f"  {left:>4}-{right:<4}: {'â–ˆ' * bar_length}{count:>3}")
    
    # Analyse du sentiment
    print_subsection("ğŸ­ ANALYSE DU SENTIMENT")
    polarities = train_df['polarity'].values
    
    # CrÃ©er des bins pour la polaritÃ©
    very_neg = (polarities < -0.3).sum()
    neg = ((polarities >= -0.3) & (polarities < -0.1)).sum()
    neutral = ((polarities >= -0.1) & (polarities <= 0.1)).sum()
    pos = ((polarities > 0.1) & (polarities <= 0.3)).sum()
    very_pos = (polarities > 0.3).sum()
    
    print("Distribution de la polaritÃ©:")
    print(f"  TrÃ¨s nÃ©gatif (<-0.3): {'â–ˆ' * very_neg}{very_neg:>3}")
    print(f"  NÃ©gatif (-0.3 Ã  -0.1): {'â–ˆ' * neg}{neg:>3}")
    print(f"  Neutre (-0.1 Ã  0.1):   {'â–ˆ' * neutral}{neutral:>3}")
    print(f"  Positif (0.1 Ã  0.3):   {'â–ˆ' * pos}{pos:>3}")
    print(f"  TrÃ¨s positif (>0.3):   {'â–ˆ' * very_pos}{very_pos:>3}")

def show_error_analysis(train_df, test_df):
    """Analyse dÃ©taillÃ©e des erreurs dÃ©tectÃ©es"""
    print_section("ğŸš¨ ANALYSE DÃ‰TAILLÃ‰E DES ERREURS", "=")
    
    train_errors = train_df[train_df['is_label_suspect'] == True]
    test_errors = test_df[test_df['is_label_suspect'] == True]
    
    total_errors = len(train_errors) + len(test_errors)
    total_samples = len(train_df) + len(test_df)
    
    print(f"ğŸ“Š RÃ‰SUMÃ‰ DES ERREURS:")
    print(f"  â€¢ Total d'erreurs dÃ©tectÃ©es: {total_errors}/{total_samples} ({total_errors/total_samples*100:.1f}%)")
    print(f"  â€¢ Erreurs dans train: {len(train_errors)}/{len(train_df)} ({len(train_errors)/len(train_df)*100:.1f}%)")
    print(f"  â€¢ Erreurs dans test: {len(test_errors)}/{len(test_df)} ({len(test_errors)/len(test_df)*100:.1f}%)")
    
    if total_errors == 0:
        print("\nâœ… EXCELLENT! Aucune erreur de label dÃ©tectÃ©e dans ce mini Ã©chantillon.")
        print("   Cela suggÃ¨re que la qualitÃ© des labels est bonne ou que les critÃ¨res")
        print("   de dÃ©tection sont appropriÃ©s.")
        return
    
    print_subsection("ğŸ” DÃ‰TAIL DES ERREURS DÃ‰TECTÃ‰ES")
    
    all_errors = pd.concat([train_errors, test_errors]) if len(train_errors) > 0 and len(test_errors) > 0 else (train_errors if len(train_errors) > 0 else test_errors)
    
    for i, (_, row) in enumerate(all_errors.iterrows()):
        print(f"\nğŸš¨ ERREUR #{i+1}")
        print("â”Œ" + "â”€" * 78 + "â”")
        print(f"â”‚ ğŸ·ï¸  Label original: {'POSITIF' if row['label'] == 1 else 'NÃ‰GATIF':<60} â”‚")
        print(f"â”‚ ğŸ­ PolaritÃ© calculÃ©e: {row['polarity']:.3f} (confiance: {row['confidence_score']:.3f}){'':>30} â”‚")
        print(f"â”‚ ğŸ’¡ Label suggÃ©rÃ©: {'POSITIF' if row['suggested_label'] == 1 else 'NÃ‰GATIF'}{'':>55} â”‚")
        print(f"â”‚ âš ï¸  Raison: {row['suspect_reason']:<65} â”‚")
        print("â”œ" + "â”€" * 78 + "â”¤")
        print(f"â”‚ ğŸ“– TITRE: {row['title'][:70]:<70} â”‚")
        print(f"â”‚ ğŸ“„ CONTENU: {row['content'][:67]:<67} â”‚")
        print("â””" + "â”€" * 78 + "â”˜")
        
        # Explication de pourquoi c'est suspect
        print("ğŸ’­ EXPLICATION:")
        if row['label'] == 1 and row['polarity'] < -0.3:
            print("   Le texte est Ã©tiquetÃ© comme POSITIF mais l'analyse de sentiment")
            print("   montre une polaritÃ© trÃ¨s nÃ©gative. Cela pourrait indiquer:")
            print("   - Une erreur d'Ã©tiquetage")
            print("   - Un sarcasme ou ironie non dÃ©tectÃ©e")
            print("   - Un contexte particulier non capturÃ© par l'analyse")
        elif row['label'] == 0 and row['polarity'] > 0.3:
            print("   Le texte est Ã©tiquetÃ© comme NÃ‰GATIF mais l'analyse de sentiment")
            print("   montre une polaritÃ© trÃ¨s positive. Cela pourrait indiquer:")
            print("   - Une erreur d'Ã©tiquetage")
            print("   - Un texte avec critique constructive")
            print("   - Une nuance non dÃ©tectÃ©e par l'analyse automatique")

def show_preprocessing_examples(train_df, num_examples=3):
    """Montre des exemples dÃ©taillÃ©s de preprocessing"""
    print_section("ğŸ” EXEMPLES DÃ‰TAILLÃ‰S DE PREPROCESSING", "=")
    
    print("Ces exemples montrent comment le texte est transformÃ© Ã©tape par Ã©tape:")
    print("â€¢ Suppression de la ponctuation excessive")
    print("â€¢ Conversion en minuscules")
    print("â€¢ Suppression des mots vides")
    print("â€¢ Lemmatisation")
    print("â€¢ Nettoyage des espaces")
    
    for i in range(min(num_examples, len(train_df))):
        row = train_df.iloc[i]
        
        print(f"\n{'='*80}")
        print(f"EXEMPLE #{i+1} - Label: {'âœ… POSITIF' if row['label'] == 1 else 'âŒ NÃ‰GATIF'}")
        print(f"{'='*80}")
        
        print(f"\nğŸ“ TITRE ORIGINAL:")
        print(f"   {row['title']}")
        
        print(f"\nğŸ“„ CONTENU ORIGINAL:")
        print(f"   {row['content'][:200]}{'...' if len(row['content']) > 200 else ''}")
        
        print(f"\nğŸ”— TEXTE COMBINÃ‰ ({len(row['combined_text'])} caractÃ¨res):")
        print(f"   {row['combined_text'][:300]}{'...' if len(row['combined_text']) > 300 else ''}")
        
        print(f"\nâœ¨ TEXTE PREPROCESSÃ‰ ({len(row['processed_text'])} caractÃ¨res):")
        print(f"   {row['processed_text'][:300]}{'...' if len(row['processed_text']) > 300 else ''}")
        
        print(f"\nğŸ“Š STATISTIQUES DE TRANSFORMATION:")
        compression = (1 - len(row['processed_text']) / len(row['combined_text'])) * 100
        print(f"   â€¢ Compression: {compression:.1f}% ({len(row['combined_text'])} â†’ {len(row['processed_text'])} chars)")
        print(f"   â€¢ Mots: {row['word_count']}")
        print(f"   â€¢ Phrases: {row['sentence_count']}")
        print(f"   â€¢ Points d'exclamation: {row['exclamation_count']}")
        print(f"   â€¢ Points d'interrogation: {row['question_count']}")
        print(f"   â€¢ Ratio majuscules: {row['upper_case_ratio']:.3f}")
        print(f"   â€¢ Ratio ponctuation: {row['punctuation_ratio']:.3f}")
        print(f"   â€¢ Ratio mots uniques: {row['unique_word_ratio']:.3f}")
        
        print(f"\nğŸ­ ANALYSE DE SENTIMENT:")
        print(f"   â€¢ PolaritÃ©: {row['polarity']:.3f} ({'Positif' if row['polarity'] > 0 else 'NÃ©gatif' if row['polarity'] < 0 else 'Neutre'})")
        print(f"   â€¢ SubjectivitÃ©: {row['subjectivity']:.3f} ({'Subjectif' if row['subjectivity'] > 0.5 else 'Objectif'})")
        
        if row['is_label_suspect']:
            print(f"\nâš ï¸  ALERTE QUALITÃ‰:")
            print(f"   â€¢ Erreur potentielle dÃ©tectÃ©e: {row['suspect_reason']}")
            print(f"   â€¢ Label suggÃ©rÃ©: {'POSITIF' if row['suggested_label'] == 1 else 'NÃ‰GATIF'}")

def test_mini_sample_visual():
    """
    Test le pipeline sur seulement 50 Ã©chantillons avec explications visuelles dÃ©taillÃ©es
    """
    print_section("ğŸ§ª TEST MINI-Ã‰CHANTILLON AVEC EXPLICATIONS VISUELLES", "=")
    
    print("Ce test va:")
    print("â€¢ âœ… Charger 50 Ã©chantillons d'entraÃ®nement + 10 de test")
    print("â€¢ âœ… Appliquer tout le pipeline de preprocessing")
    print("â€¢ âœ… CrÃ©er des sauvegardes multiples")
    print("â€¢ âœ… Analyser les rÃ©sultats en dÃ©tail")
    print("â€¢ âœ… DÃ©tecter les erreurs potentielles")
    print("â€¢ âœ… Fournir des explications visuelles")
    
    # Configuration pour mini test
    output_dir = "data/mini_test_visual"
    pipeline = MassivePreprocessingPipeline(
        output_dir=output_dir,
        use_multiprocessing=False,  # Pas de multiprocessing pour 50 Ã©chantillons
        chunk_size=25  # TrÃ¨s petits chunks
    )
    
    # CrÃ©er le dossier de sauvegarde
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    print_section("ğŸ“¥ CHARGEMENT DES DONNÃ‰ES", "=")
    
    # Charger seulement 50 Ã©chantillons
    from datasets import load_dataset
    
    print("ğŸ”„ Chargement d'un mini Ã©chantillon depuis HuggingFace...")
    start_time = time.time()
    
    # Charger seulement 50 Ã©chantillons train et 10 test
    train_dataset = load_dataset("fancyzhx/amazon_polarity", split="train[:50]")
    test_dataset = load_dataset("fancyzhx/amazon_polarity", split="test[:10]")
    
    load_time = time.time() - start_time
    print(f"âœ… Mini Ã©chantillon chargÃ© en {load_time:.1f}s")
    print(f"   â€¢ Train: {len(train_dataset)} Ã©chantillons")
    print(f"   â€¢ Test: {len(test_dataset)} Ã©chantillons")
    print(f"   â€¢ Dataset: Amazon Polarity (reviews de produits)")
    print(f"   â€¢ Labels: 0=NÃ©gatif, 1=Positif")
    
    # Montrer quelques exemples bruts
    print_subsection("ğŸ‘€ APERÃ‡U DES DONNÃ‰ES BRUTES")
    for i in range(min(3, len(train_dataset))):
        sample = train_dataset[i]
        print(f"\nğŸ“¦ Ã‰chantillon {i+1}:")
        print(f"   ğŸ·ï¸  Label: {'âœ… POSITIF' if sample['label'] == 1 else 'âŒ NÃ‰GATIF'}")
        print(f"   ğŸ“– Titre: {sample['title']}")
        print(f"   ğŸ“„ Contenu: {sample['content'][:100]}...")
    
    # SAUVEGARDE 1: DonnÃ©es brutes
    print_section("ğŸ’¾ SAUVEGARDE 1: DONNÃ‰ES BRUTES", "=")
    raw_train = pd.DataFrame(train_dataset)
    raw_test = pd.DataFrame(test_dataset)
    
    raw_train.to_csv(f"{output_dir}/raw_train_backup.csv", index=False)
    raw_test.to_csv(f"{output_dir}/raw_test_backup.csv", index=False)
    print(f"âœ… SauvegardÃ©: raw_train_backup.csv ({len(raw_train)} lignes)")
    print(f"âœ… SauvegardÃ©: raw_test_backup.csv ({len(raw_test)} lignes)")
    
    # Traitement des donnÃ©es
    print_section("ğŸ”§ TRAITEMENT DU MINI TRAIN", "=")
    print("Application du pipeline complet:")
    print("â€¢ Combinaison titre + contenu")
    print("â€¢ Nettoyage et normalisation du texte")
    print("â€¢ Extraction de features linguistiques")
    print("â€¢ Analyse de sentiment")
    print("â€¢ DÃ©tection d'erreurs de labels")
    
    train_df = pipeline.process_dataset_split(train_dataset, 'mini_train')
    
    # SAUVEGARDE 2: Train preprocessÃ©
    print_section("ğŸ’¾ SAUVEGARDE 2: TRAIN PREPROCESSÃ‰", "=")
    train_df.to_csv(f"{output_dir}/processed_train_backup.csv", index=False)
    train_df.to_parquet(f"{output_dir}/processed_train_backup.parquet", index=False)
    print(f"âœ… SauvegardÃ©: processed_train_backup.csv ({len(train_df)} lignes)")
    print(f"âœ… SauvegardÃ©: processed_train_backup.parquet")
    print(f"ğŸ“Š Features crÃ©Ã©es: {len(train_df.columns)} colonnes")
    
    print_section("ğŸ”§ TRAITEMENT DU MINI TEST", "=")
    test_df = pipeline.process_dataset_split(test_dataset, 'mini_test')
    
    # SAUVEGARDE 3: Test preprocessÃ©
    print_section("ğŸ’¾ SAUVEGARDE 3: TEST PREPROCESSÃ‰", "=")
    test_df.to_csv(f"{output_dir}/processed_test_backup.csv", index=False)
    test_df.to_parquet(f"{output_dir}/processed_test_backup.parquet", index=False)
    print(f"âœ… SauvegardÃ©: processed_test_backup.csv ({len(test_df)} lignes)")
    print(f"âœ… SauvegardÃ©: processed_test_backup.parquet")
    
    # Sauvegarder via le pipeline standard
    pipeline.save_processed_data(train_df, test_df)
    
    # CrÃ©er des statistiques visuelles
    create_visual_stats(train_df, test_df)
    
    # Analyser les erreurs
    show_error_analysis(train_df, test_df)
    
    # Montrer des exemples de preprocessing
    show_preprocessing_examples(train_df, num_examples=3)
    
    # RÃ©sumÃ© final avec style
    print_section("ğŸ¯ RÃ‰SUMÃ‰ FINAL", "=")
    
    total_errors = train_df['is_label_suspect'].sum() + test_df['is_label_suspect'].sum()
    compression_ratio = (1 - train_df['processed_length'].mean() / train_df['text_length'].mean()) * 100
    
    print("ğŸ“Š DONNÃ‰ES TRAITÃ‰ES:")
    print(f"   âœ… Train: {len(train_df)} Ã©chantillons")
    print(f"   âœ… Test: {len(test_df)} Ã©chantillons")
    print(f"   âœ… Features: {len(train_df.columns)} colonnes crÃ©Ã©es")
    
    print("\nğŸ”§ PERFORMANCE DU PREPROCESSING:")
    print(f"   âœ… Compression moyenne: {compression_ratio:.1f}%")
    print(f"   âœ… Mots moyens par Ã©chantillon: {train_df['word_count'].mean():.1f}")
    print(f"   âœ… Phrases moyennes par Ã©chantillon: {train_df['sentence_count'].mean():.1f}")
    
    print("\nğŸ­ ANALYSE DE SENTIMENT:")
    print(f"   âœ… PolaritÃ© moyenne: {train_df['polarity'].mean():.3f}")
    print(f"   âœ… SubjectivitÃ© moyenne: {train_df['subjectivity'].mean():.3f}")
    
    print("\nğŸš¨ CONTRÃ”LE QUALITÃ‰:")
    if total_errors > 0:
        print(f"   âš ï¸  {total_errors} erreur(s) potentielle(s) dÃ©tectÃ©e(s)")
        print(f"   ğŸ“Š Taux d'erreur: {total_errors/(len(train_df)+len(test_df))*100:.1f}%")
        print("   ğŸ’¡ Le systÃ¨me de dÃ©tection fonctionne correctement")
    else:
        print("   âœ… Aucune erreur dÃ©tectÃ©e dans ce mini Ã©chantillon")
        print("   ğŸ’¡ QualitÃ© des labels semble bonne")
    
    print("\nğŸ’¾ FICHIERS CRÃ‰Ã‰S:")
    print(f"   ğŸ“ Dossier: {output_dir}/")
    print("   âœ… DonnÃ©es brutes (CSV)")
    print("   âœ… DonnÃ©es preprocessÃ©es (CSV + Parquet + Pickle)")
    print("   âœ… Statistiques et analyses (JSON)")
    
    print_section("ğŸš€ PROCHAINES Ã‰TAPES", "=")
    print("1ï¸âƒ£  Examiner les fichiers crÃ©Ã©s pour comprendre les transformations")
    print("2ï¸âƒ£  Lancer: python analyze_mini_results.py (pour plus d'analyses)")
    print("3ï¸âƒ£  Si satisfait, tester sur plus d'Ã©chantillons:")
    print("     â€¢ python test_massive_pipeline.py (10k Ã©chantillons)")
    print("     â€¢ python run_full_preprocessing.py (4M Ã©chantillons)")
    
    print_section("âœ… MINI TEST VISUEL TERMINÃ‰ AVEC SUCCÃˆS!", "=")
    
    return train_df, test_df

def main():
    """
    Lance le mini test visuel
    """
    try:
        print("ğŸš€ LANCEMENT DU MINI TEST AVEC EXPLICATIONS VISUELLES")
        
        train_df, test_df = test_mini_sample_visual()
        
        print(f"\nğŸ‰ MINI TEST VISUEL RÃ‰USSI!")
        print(f"Le pipeline est maintenant bien documentÃ© et testÃ©.")
        
    except Exception as e:
        print(f"\nâŒ ERREUR LORS DU MINI TEST VISUEL: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 